
# you must set worker processes based on your CPU cores, nginx does not benefit from setting more than that
worker_processes auto; #some last versions calculate it automatically

# number of file descriptors used for nginx
# the limit for the maximum FDs on the server is usually set by the OS.
# if you don't set FD's then OS settings will be used which is by default 2000
worker_rlimit_nofile 100000;

# only log critical errors
error_log /var/log/nginx/error.log crit;

# provides the configuration file context in which the directives that affect connection processing are specified.
events {
    # determines how much clients will be served per worker
    # max clients = worker_connections * worker_processes
    # max clients is also limited by the number of socket connections available on the system (~64k)
    worker_connections 4000;

    # optimized to serve many clients with each thread, essential for linux -- for testing environment
    use epoll;

    # accept as many connections as possible, may flood worker connections if set too low -- for testing environment
    multi_accept on;
}

http {
  # cache informations about FDs, frequently accessed files
  # can boost performance, but you need to test those values
  open_file_cache max=200000 inactive=20s;
  open_file_cache_valid 30s;
  open_file_cache_min_uses 2;
  open_file_cache_errors on;

  # to boost I/O on HDD we can disable access logs
  log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

  # access_log off;
  access_log  /var/log/nginx/access.log  main;

  # copies data between one FD and other from within the kernel
  # faster than read() + write()
  sendfile on;

  # send headers in one piece, it is better than sending them one by one
  tcp_nopush on;

  # don't buffer data sent, good for small data bursts in real time
  tcp_nodelay on;

  # reduce the data that needs to be sent over network -- for testing environment
  gzip on;
  # gzip_static on;
  gzip_min_length 10240;
  gzip_comp_level 1;
  gzip_vary on;
  gzip_disable msie6;
  gzip_proxied expired no-cache no-store private auth;
  gzip_types
      # text/html is always compressed by HttpGzipModule
      text/css
      text/javascript
      text/xml
      text/plain
      text/x-component
      application/javascript
      application/x-javascript
      application/json
      application/xml
      application/rss+xml
      application/atom+xml
      font/truetype
      font/opentype
      application/vnd.ms-fontobject
      image/svg+xml;

  # allow the server to close connection on non responding client, this will free up memory
  reset_timedout_connection on;

  # request timed out -- default 60
  client_body_timeout 10;

  # if client stop responding, free up memory -- default 60
  send_timeout 2;

  # server will close connection after this time -- default 75
  keepalive_timeout 30;

  # number of requests client can make over keep-alive -- for testing environment
  keepalive_requests 100000;



  

  # LOAD_BALANCING

  # Define which servers to include in the load balancing scheme. 
  # It's best to use the servers' private IPs for better performance and security.
  upstream xmpp {

    # Load balancing methods: round-robin; least_conn; ip_hash; By default, this value is set to round-robin.
    # IP hash method is prefferable, but Nginx also supports session persistence using cookies in Nginx Plus
    least_conn;

    # 1. Priority: server with the highest weight param is selected the most often.
    # 2. Health check: max_fails and fail_timeout params.
    # 2.1 max_fails: the number of consecutive unsuccessful connection attempts(1 by default, 0 will disable health checks to that server).
    # 2.2 fail_timeout: the time the server is considered failed and, in case of max_fails > 1, the timeout within all max_fails should be happen(10s by default).
    # 2.3 custom health conditions are a feature of active health monitoring and exclusive to NGINX PLUS.
    # 3. Its also possible load balance on same node between multiple ports
    # 4. To add another backend server on the fly without having to restart the load balancer you could preconfigure
    # the backend with predictable IP addresses, for example, by using an SDN Private Network and deploying new backend
    # servers from a template. The best results would require using the API along with the load monitoring of your choice.
    server 127.0.0.1:3002 weight=4;
    server 127.0.0.1:3003 weight=2;
    server 127.0.0.1:3005 max_fails=3 fail_timeout=30s;
  }

  # include servers
  include /etc/nginx/conf.d/*.conf;

  # You can monitoring nginx with Amplify
}